{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGWKQgaWxAlG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross validation for reddit meta model and twitter dataset"
      ],
      "metadata": {
        "id": "Llp0rq-m-lE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the twitter and bitcoin merged dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/FYP/dataset/Merged Dataset/twitter_bitcoin_merged.csv', parse_dates=True)\n",
        "data = data.rename(columns={'Unnamed: 0': 'timestamp'})\n",
        "data = data.drop(columns=['compound', 'polarity', 'subjectivity', 'open', 'high', 'low'])\n",
        "data.set_index('timestamp', inplace=True)\n",
        "\n",
        "# Shift the \"close\" column 1 hour into the future and make it the target variable\n",
        "data[\"target\"] = data[\"close\"].shift(-1)\n",
        "data = data.iloc[:-1]\n",
        "\n",
        "# Drop missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Selecting features\n",
        "features = ['pos', 'neg', 'neu', 'close', 'volume']\n",
        "data[features]"
      ],
      "metadata": {
        "id": "Q55BV8AkxMSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise the dataframe\n",
        "data"
      ],
      "metadata": {
        "id": "ZRloUtbR4XIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into features and target\n",
        "X = data.drop('target', axis=1).values\n",
        "y = data['target'].values.reshape(-1, 1)\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "train_size = int(0.8 * len(X))\n",
        "test_size = len(X) - train_size \n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Scale the data\n",
        "scaler_X = MinMaxScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_test_scaled = scaler_y.transform(y_test)\n",
        "\n",
        "# Reshape input to be 3D [samples, timesteps, features]\n",
        "n_features = X.shape[1]\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, n_features))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, n_features))"
      ],
      "metadata": {
        "id": "IH5vawPzxnOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the five saved models\n",
        "model_rnn = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_rnn.h5')\n",
        "model_lstm = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_lstm.h5')\n",
        "model_gru = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_gru.h5')\n",
        "model_bilstm = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_bilstm.h5')\n",
        "model_bigru = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_bigru.h5')"
      ],
      "metadata": {
        "id": "bkxlD2OQBd-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions from the five models\n",
        "preds_test_rnn = model_rnn.predict(X_test_reshaped)\n",
        "preds_test_lstm = model_lstm.predict(X_test_reshaped)\n",
        "preds_test_gru = model_gru.predict(X_test_reshaped)\n",
        "preds_test_bilstm = model_bilstm.predict(X_test_reshaped)\n",
        "preds_test_bigru = model_bigru.predict(X_test_reshaped)\n",
        "\n",
        "# Stack the predictions into a single matrix\n",
        "base_preds_test = np.column_stack((preds_test_rnn, preds_test_lstm, preds_test_gru, preds_test_bilstm, preds_test_bigru))"
      ],
      "metadata": {
        "id": "VqGSRFmtzQnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the reddit meta model\n",
        "with open('/content/drive/MyDrive/FYP/ensemble_models/5_feature/reddit/lr_test_meta_model.pkl', 'rb') as f:\n",
        "    reddit_model = pickle.load(f)"
      ],
      "metadata": {
        "id": "H0lq8_rBzUz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions from the five models\n",
        "preds_test_meta = reddit_model.predict(base_preds_test)\n"
      ],
      "metadata": {
        "id": "wyBcuwFizaZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse transforming the data\n",
        "meta_predict_test_inv = scaler_y.inverse_transform(preds_test_meta)\n",
        "y_test_actual = scaler_y.inverse_transform(y_test_scaled)"
      ],
      "metadata": {
        "id": "4A_LHUNp2UY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance of your meta model\n",
        "mse = np.sqrt(mean_squared_error(y_test_actual, meta_predict_test_inv, squared=False))\n",
        "mae = mean_absolute_error(y_test_actual, meta_predict_test_inv)\n",
        "r2 = r2_score(y_test_actual, meta_predict_test_inv)\n",
        "\n",
        "print(mse)\n",
        "print(mae)\n",
        "print(r2)"
      ],
      "metadata": {
        "id": "Ea4IHoi9za1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross validation for twitter model and reddit dataset"
      ],
      "metadata": {
        "id": "NExT3fXc-2VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the twitter and bitcoin merged dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/FYP/dataset/Merged Dataset/reddit_bitcoin_merged.csv', parse_dates=True)\n",
        "data = data.rename(columns={'Unnamed: 0': 'timestamp'})\n",
        "data = data.drop(columns=['flair','compound', 'polarity', 'subjectivity', 'open', 'high', 'low'])\n",
        "data.set_index('timestamp', inplace=True)\n",
        "\n",
        "# Shift the \"close\" column 1 hour into the future and make it the target variable\n",
        "data[\"target\"] = data[\"close\"].shift(-1)\n",
        "data = data.iloc[:-1]\n",
        "\n",
        "# Drop missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Selecting features\n",
        "features = ['pos', 'neg', 'neu', 'close', 'volume']\n",
        "data[features]"
      ],
      "metadata": {
        "id": "GnWZfD2S-9Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise the dataframe\n",
        "data"
      ],
      "metadata": {
        "id": "AnfKyN0s_Acn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into features and target\n",
        "X = data.drop('target', axis=1).values\n",
        "y = data['target'].values.reshape(-1, 1)\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "train_size = int(0.8 * len(X))\n",
        "test_size = len(X) - train_size \n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Scale the data\n",
        "scaler_X = MinMaxScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_test_scaled = scaler_y.transform(y_test)A\n",
        "\n",
        "# Reshape input to be 3D [samples, timesteps, features]\n",
        "n_features = X.shape[1]\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, n_features))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, n_features))"
      ],
      "metadata": {
        "id": "7LFAy6ka_C6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the five saved models\n",
        "model_rnn = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_rnn.h5')\n",
        "model_lstm = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_lstm.h5')\n",
        "model_gru = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_gru.h5')\n",
        "model_bilstm = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_bilstm.h5')\n",
        "model_bigru = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_bigru.h5')\n"
      ],
      "metadata": {
        "id": "TKECgd7D_FDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the five saved models\n",
        "model_rnn = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/twitter_rnn.h5')\n",
        "model_lstm = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/twitter_lstm.h5')\n",
        "model_gru = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/twitter_gru.h5')\n",
        "model_bilstm = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/twitter_bilstm.h5')\n",
        "model_bigru = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/twitter_bigru.h5')"
      ],
      "metadata": {
        "id": "CglL-PHnBgH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions from the five models\n",
        "preds_test_rnn = model_rnn.predict(X_test_reshaped)\n",
        "preds_test_lstm = model_lstm.predict(X_test_reshaped)\n",
        "preds_test_gru = model_gru.predict(X_test_reshaped)\n",
        "preds_test_bilstm = model_bilstm.predict(X_test_reshaped)\n",
        "preds_test_bigru = model_bigru.predict(X_test_reshaped)\n",
        "\n",
        "# Stack the predictions into a single matrix\n",
        "base_preds_test = np.column_stack((preds_test_rnn, preds_test_lstm, preds_test_gru, preds_test_bilstm, preds_test_bigru))"
      ],
      "metadata": {
        "id": "OE9Aj7UA_LUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the reddit meta model\n",
        "with open('/content/drive/MyDrive/FYP/ensemble_models/5_feature/twitter/lr_test_meta_model.pkl', 'rb') as f:\n",
        "    twitter_model = pickle.load(f)"
      ],
      "metadata": {
        "id": "DZN1NIsw_OQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions from the five models\n",
        "preds_test_meta = twitter_model.predict(base_preds_test)\n"
      ],
      "metadata": {
        "id": "K2H97sbU_Sq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse transforming the data\n",
        "meta_predict_test_inv = scaler_y.inverse_transform(preds_test_meta)\n",
        "y_test_actual = scaler_y.inverse_transform(y_test_scaled)"
      ],
      "metadata": {
        "id": "BFtsV47o_Uav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance of your meta model\n",
        "mse = np.sqrt(mean_squared_error(y_test_actual, meta_predict_test_inv, squared=False))\n",
        "mae = mean_absolute_error(y_test_actual, meta_predict_test_inv)\n",
        "r2 = r2_score(y_test_actual, meta_predict_test_inv)\n",
        "\n",
        "print(mse)\n",
        "print(mae)\n",
        "print(r2)"
      ],
      "metadata": {
        "id": "ylRFPj4O_Wdw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}