{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_kSeoQp3ZyEe"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries and functions"
      ],
      "metadata": {
        "id": "BsKz5d9TMK0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import pickle\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n"
      ],
      "metadata": {
        "id": "_Ma5gf3QMTj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading, preprocessing and scaling the data"
      ],
      "metadata": {
        "id": "apYLMXsPnZPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reddit and bitcoin merged data"
      ],
      "metadata": {
        "id": "rHxqyQxfZaYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the reddit amd bitcoin merged data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/FYP/dataset/Merged Dataset/reddit_bitcoin_merged.csv\", index_col='timestamp', parse_dates=True)\n",
        "\n",
        "data = data.drop(columns=['flair', 'compound', 'polarity', 'subjectivity', 'open', 'high', 'low'])"
      ],
      "metadata": {
        "id": "Wyb57r8rWR_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Twitter and bitcoin merged data"
      ],
      "metadata": {
        "id": "pPeZKmF3Zf73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the twitter and bitcoin merged dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/FYP/dataset/Merged Dataset/twitter_bitcoin_merged.csv', parse_dates=True)\n",
        "data = data.rename(columns={'Unnamed: 0': 'timestamp'})\n",
        "data = data.drop(columns=['compound', 'polarity', 'subjectivity', 'open', 'high', 'low'])\n",
        "data.set_index('timestamp', inplace=True)"
      ],
      "metadata": {
        "id": "UvttoMvqUzzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shift the \"close\" column 1 hour into the future and make it the target variable\n",
        "data[\"target\"] = data[\"close\"].shift(-1)\n",
        "data = data.iloc[:-1]\n",
        "\n",
        "# Drop missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Visualise the dataframe\n",
        "data"
      ],
      "metadata": {
        "id": "msEUiNHDWQFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['pos', 'neg', 'neu', 'close', 'volume']\n",
        "data[features]\n"
      ],
      "metadata": {
        "id": "cWwLDGFNAKoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into features and target\n",
        "X = data.drop('target', axis=1).values\n",
        "y = data['target'].values.reshape(-1, 1)\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "train_size = int(0.8 * len(X))\n",
        "test_size = len(X) - train_size \n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Scale the data\n",
        "scaler_X = MinMaxScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_test_scaled = scaler_y.transform(y_test)\n",
        "\n",
        "# Reshape input to be 3D [samples, timesteps, features]\n",
        "n_features = X.shape[1]\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, n_features))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, n_features))"
      ],
      "metadata": {
        "id": "-_G0NQ_tAXqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('No. of features', n_features)\n",
        "print(X_train_reshaped)\n",
        "print(X_test_reshaped)"
      ],
      "metadata": {
        "id": "IWZG0tbirWE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading, training and testing the base models"
      ],
      "metadata": {
        "id": "WPwXgi95njLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reddit RNN base laerners"
      ],
      "metadata": {
        "id": "yD_ob_w0VcMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the five saved models\n",
        "model_rnn = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_rnn.h5')\n",
        "model_lstm = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_lstm.h5')\n",
        "model_gru = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_gru.h5')\n",
        "model_bilstm = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_bilstm.h5')\n",
        "model_bigru = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/reddit_bigru.h5')\n"
      ],
      "metadata": {
        "id": "KDb1kG_vT0cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Twitter RNN base learners"
      ],
      "metadata": {
        "id": "ur6gKCu3VkV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the five saved models\n",
        "model_rnn = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/twitter_rnn.h5')\n",
        "model_lstm = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/twitter_lstm.h5')\n",
        "model_gru = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/twitter_gru.h5')\n",
        "model_bilstm = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/twitter_bilstm.h5')\n",
        "model_bigru = load_model('/content/drive/MyDrive/FYP/rnn_base_models/5_features/twitter_bigru.h5')\n"
      ],
      "metadata": {
        "id": "xLrCPuquVpCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainig base models "
      ],
      "metadata": {
        "id": "Osvk6KMMY0HG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions from the five models\n",
        "preds_train_rnn = model_rnn.predict(X_train_reshaped)\n",
        "preds_train_lstm = model_lstm.predict(X_train_reshaped)\n",
        "preds_train_gru = model_gru.predict(X_train_reshaped)\n",
        "preds_train_bilstm = model_bilstm.predict(X_train_reshaped)\n",
        "preds_train_bigru = model_bigru.predict(X_train_reshaped)\n",
        "\n",
        "# Stack the predictions into a single matrix\n",
        "base_preds_train = np.column_stack((preds_train_rnn, preds_train_lstm, preds_train_gru, preds_train_bilstm, preds_train_bigru))\n"
      ],
      "metadata": {
        "id": "lQmZvrLhT2GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test base models"
      ],
      "metadata": {
        "id": "GH5PotYCY5MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions from the five models\n",
        "preds_test_rnn = model_rnn.predict(X_test_reshaped)\n",
        "preds_test_lstm = model_lstm.predict(X_test_reshaped)\n",
        "preds_test_gru = model_gru.predict(X_test_reshaped)\n",
        "preds_test_bilstm = model_bilstm.predict(X_test_reshaped)\n",
        "preds_test_bigru = model_bigru.predict(X_test_reshaped)\n",
        "\n",
        "# Stack the predictions into a single matrix\n",
        "base_preds_test = np.column_stack((preds_test_rnn, preds_test_lstm, preds_test_gru, preds_test_bilstm, preds_test_bigru))"
      ],
      "metadata": {
        "id": "LrhvIuYhT3bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking the base models to produce meta model"
      ],
      "metadata": {
        "id": "VMzJ200inuYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the regression meta models"
      ],
      "metadata": {
        "id": "c_cnwc-7yjfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stacked_ensemble(meta_model, base_preds_train, y_train_scaled, base_preds_test, y_test_scaled, save_path=None):\n",
        "    # Fit the meta-model\n",
        "    meta_model.fit(base_preds_train, y_train_scaled)\n",
        "\n",
        "    # Generate predictions from the meta-model\n",
        "    meta_preds_test = meta_model.predict(base_preds_test)\n",
        "\n",
        "    # Reshape the predictions to be two-dimensional\n",
        "    meta_preds_test = meta_preds_test.reshape(-1, 1)\n",
        "\n",
        "    # Inverse transforming the scaled data\n",
        "    meta_predict_test_inv = scaler_y.inverse_transform(meta_preds_test)\n",
        "    y_test_actual = scaler_y.inverse_transform(y_test_scaled)\n",
        "\n",
        "    # Calculate the mean squared error, mean absolute error, and r2 score\n",
        "    mse = np.sqrt(mean_squared_error(y_test_actual, meta_predict_test_inv, squared=False))\n",
        "    mae = mean_absolute_error(y_test_actual, meta_predict_test_inv)\n",
        "    r2 = r2_score(y_test_actual, meta_predict_test_inv)\n",
        "\n",
        "    # Save the model to a file\n",
        "    if save_path is not None:\n",
        "        with open(save_path, 'wb') as file:\n",
        "            pickle.dump(meta_model, file)\n",
        "\n",
        "    # Return a dictionary containing the model, model predictions, and evaluation metrics\n",
        "    results = {\n",
        "        'model': meta_model,\n",
        "        'mse': mse,\n",
        "        'mae': mae,\n",
        "        'r2': r2\n",
        "    }\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "k33LgnMoMCk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random forest"
      ],
      "metadata": {
        "id": "QZwzePchjbAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a random forest regression model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=50)\n",
        "\n",
        "# Evaluate random forest model and save the trained meta-model\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/twitter/rf_test_meta_model.pkl'\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/reddit/rf_test_meta_model.pkl'\n",
        "\n",
        "# rf_eval = evaluate_ensemble(rf_model, base_preds_train, y_train, base_preds_test, y_test, save_path=meta_model_path)\n",
        "rf_eval = stacked_ensemble(rf_model, base_preds_train, y_train_scaled, base_preds_test, y_test_scaled, save_path=meta_model_path)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation metrics for the ensemble:\")\n",
        "for metric_name, metric_value in rf_eval.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")\n"
      ],
      "metadata": {
        "id": "XtowM06pT4z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression "
      ],
      "metadata": {
        "id": "_isnijopjeia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a linear regression model\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "# Evaluate random forest model and save the trained meta-model\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/twitter/lr_test_meta_model.pkl'\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/reddit/lr_test_meta_model.pkl'\n",
        "\n",
        "# lr_eval = evaluate_ensemble(lr_model, base_preds_train, y_train, base_preds_test, y_test, save_path=meta_model_path)\n",
        "lr_eval = stacked_ensemble(lr_model, base_preds_train, y_train_scaled, base_preds_test, y_test_scaled, save_path=meta_model_path)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation metrics for the ensemble:\")\n",
        "for metric_name, metric_value in lr_eval.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")"
      ],
      "metadata": {
        "id": "u5H6as2ejDbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elastic net regression"
      ],
      "metadata": {
        "id": "ehofXTbfkfZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a elastic net regression model\n",
        "en_model = ElasticNet(alpha=0.015, l1_ratio=0.5, random_state=70)\n",
        "\n",
        "# Evaluate random forest model and save the trained meta-model\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/twitter/en_test_meta_model.pkl'\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/reddit/en_test_meta_model.pkl'\n",
        "\n",
        "en_eval = stacked_ensemble(en_model, base_preds_train, y_train_scaled, base_preds_test, y_test_scaled, save_path=meta_model_path)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation metrics for the ensemble:\")\n",
        "for metric_name, metric_value in en_eval.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")"
      ],
      "metadata": {
        "id": "-RMEFNr7ki6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support vector regression"
      ],
      "metadata": {
        "id": "BTksFRGdmqzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a support vector regression model\n",
        "svr_model = SVR()\n",
        "\n",
        "# Evaluate random forest model and save the trained meta-model\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/twitter/svr_test_meta_model.pkl'\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/reddit/svr_test_meta_model.pkl'\n",
        "\n",
        "svr_eval = stacked_ensemble(svr_model, base_preds_train, y_train_scaled, base_preds_test, y_test_scaled, save_path=meta_model_path)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation metrics for the ensemble:\")\n",
        "for metric_name, metric_value in svr_eval.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")"
      ],
      "metadata": {
        "id": "HSLGBxAonDFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision tree regression"
      ],
      "metadata": {
        "id": "YLUC0jqQoSso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a decision tree regression model\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Evaluate random forest model and save the trained meta-model\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/twitter/dt_test_meta_model.pkl'\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/reddit/dt_test_meta_model.pkl'\n",
        "\n",
        "\n",
        "dt_eval = stacked_ensemble(dt_model, base_preds_train, y_train_scaled, base_preds_test, y_test_scaled, save_path=meta_model_path)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation metrics for the ensemble:\")\n",
        "for metric_name, metric_value in dt_eval.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")\n",
        "   "
      ],
      "metadata": {
        "id": "MemcKwDyoWLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient boosting regression"
      ],
      "metadata": {
        "id": "Cc6mPVKromA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a gradient boosting regression model\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Evaluate random forest model and save the trained meta-model\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/twitter/gb_test_meta_model.pkl'\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/reddit/gb_test_meta_model.pkl'\n",
        "\n",
        "gb_eval = stacked_ensemble(gb_model, base_preds_train, y_train_scaled, base_preds_test, y_test_scaled, save_path=meta_model_path)\n",
        "\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation metrics for the ensemble:\")\n",
        "for metric_name, metric_value in gb_eval.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")"
      ],
      "metadata": {
        "id": "chdbxeiCo2oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ada boosting regression"
      ],
      "metadata": {
        "id": "E0727XijpOio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "\n",
        "# Instantiate a adaboost regression model\n",
        "ab_model = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Evaluate random forest model and save the trained meta-model\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/twitter/ada_test_meta_model.pkl'\n",
        "# meta_model_path = '/content/drive/MyDrive/FYP/ensemble_models/5_feature/reddit/ada_test_meta_model.pkl'\n",
        "\n",
        "ab_eval = stacked_ensemble(ab_model, base_preds_train, y_train_scaled, base_preds_test, y_test_scaled, save_path=meta_model_path)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation metrics for the ensemble:\")\n",
        "for metric_name, metric_value in ab_eval.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")"
      ],
      "metadata": {
        "id": "4HYb4fEEpSIk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}